apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: geozarr-convert
  namespace: devseed
  labels:
    app.kubernetes.io/name: geozarr-convert
    app.kubernetes.io/part-of: geozarr
    managed-by: argo
    example: "true"
spec:
  entrypoint: pipeline
  serviceAccountName: devseed
  workflowMetadata:
    labels:
      app.kubernetes.io/name: geozarr-convert
      app.kubernetes.io/part-of: geozarr
      example: "true"
  arguments:
    parameters:
      - name: image
        value: "eopf-geozarr:dev"
      - name: stac_url
        value: "https://example.invalid/in.zarr"
      - name: output_zarr
        value: "/data/out.zarr"
      - name: groups
        value: "measurements/reflectance/r20m"
      - name: validate_groups
        value: "false"
      - name: aoi
        value: ""
      - name: register_url
        value: ""
      - name: register_collection
        value: ""
      - name: register_bearer_token
        value: ""
      - name: register_href
        value: ""
      # Optional S3-compatible upload (e.g., OVHcloud)
      - name: s3_endpoint
        value: "https://s3.de.io.cloud.ovh.net"
      - name: s3_bucket
        value: "esa-zarr-sentinel-explorer-fra"
      - name: s3_key
        value: ""
      - name: s3_access_key
        value: ""
      - name: s3_secret_key
        value: ""
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 50Gi
  # Tip: adjust storage size per dataset if needed.
  templates:
    - name: pipeline
      dag:
        tasks:
          - name: convert
            template: convert
            arguments:
              parameters:
                - { name: image, value: "{{workflow.parameters.image}}" }
                - { name: stac_url, value: "{{workflow.parameters.stac_url}}" }
                - { name: output_zarr, value: "{{workflow.parameters.output_zarr}}" }
                - { name: groups, value: "{{workflow.parameters.groups}}" }
                - { name: validate_groups, value: "{{workflow.parameters.validate_groups}}" }
                - { name: aoi, value: "{{workflow.parameters.aoi}}" }
          - name: upload-s3
            dependencies: [convert]
            template: upload-s3
            arguments:
              parameters:
                - { name: image, value: "{{workflow.parameters.image}}" }
                - { name: output_zarr, value: "{{workflow.parameters.output_zarr}}" }
                - { name: s3_endpoint, value: "{{workflow.parameters.s3_endpoint}}" }
                - { name: s3_bucket, value: "{{workflow.parameters.s3_bucket}}" }
                - { name: s3_key, value: "{{workflow.parameters.s3_key}}" }
                - { name: s3_access_key, value: "{{workflow.parameters.s3_access_key}}" }
                - { name: s3_secret_key, value: "{{workflow.parameters.s3_secret_key}}" }
          - name: register
            dependencies: [upload-s3]
            template: register-stac
            arguments:
              parameters:
                - { name: image, value: "{{workflow.parameters.image}}" }
                - { name: output_zarr, value: "{{workflow.parameters.output_zarr}}" }
                - { name: register_href, value: "{{workflow.parameters.register_href}}" }
                - { name: register_url, value: "{{workflow.parameters.register_url}}" }
                - { name: register_collection, value: "{{workflow.parameters.register_collection}}" }
                - { name: register_bearer_token, value: "{{workflow.parameters.register_bearer_token}}" }
                - { name: s3_endpoint, value: "{{workflow.parameters.s3_endpoint}}" }
                - { name: s3_bucket, value: "{{workflow.parameters.s3_bucket}}" }
                - { name: s3_key, value: "{{workflow.parameters.s3_key}}" }

    - name: convert
      inputs:
        parameters:
          - name: image
          - name: stac_url
          - name: output_zarr
          - name: groups
          - name: validate_groups
          - name: aoi
      container:
        image: "{{inputs.parameters.image}}"
        imagePullPolicy: IfNotPresent
        command: [/bin/bash, -lc]
        args:
          - |
            set -euo pipefail

            STAC="{{inputs.parameters.stac_url}}"
            OUT="{{inputs.parameters.output_zarr}}"
            GROUPS_RAW="{{inputs.parameters.groups}}"
            VALIDATE="{{inputs.parameters.validate_groups}}"
            AOI="{{inputs.parameters.aoi}}"
            PERF="/data/dask-report.html"

            echo "=== inputs ==="
            echo "stac_url=$STAC"
            echo "output_zarr=$OUT"
            echo "groups_raw=$GROUPS_RAW"
            echo "validate_groups=$VALIDATE"
            echo "aoi=$AOI"

            echo "=== env sanity ==="
            if ! command -v eopf-geozarr >/dev/null 2>&1; then
              echo "[error] eopf-geozarr not found on PATH";
              exit 127;
            fi

            # Normalize groups to absolute paths (accept comma or space separated)
            GROUP_LIST=""
            for g in $(printf '%s' "$GROUPS_RAW" | tr ',' ' '); do
              [ -z "$g" ] && continue
              case "$g" in /*) N="$g";; *) N="/$g";; esac
              GROUP_LIST="$GROUP_LIST $N"
            done
            GROUP_LIST="${GROUP_LIST# }"
            echo "groups_norm=$GROUP_LIST"

            if [ "$VALIDATE" = "true" ]; then
              echo "=== validate groups ==="
              for g in $GROUP_LIST; do
                if ! python -c 'import sys,xarray as xr; stac,grp=sys.argv[1:3]; dt=xr.open_datatree(stac, engine="zarr", consolidated=None); key=grp.strip("/"); _=dt[key]' "$STAC" "$g"; then
                  echo "[error] $g: missing or unreadable"
                  exit 64
                fi
              done
              echo "Groups OK: $GROUP_LIST"
            fi

            echo "=== convert ==="
            CMD=( eopf-geozarr convert "$STAC" "$OUT" --verbose )
            for g in $GROUP_LIST; do CMD+=( --groups "$g" ); done
            if [ -n "$AOI" ]; then CMD+=( --aoi "$AOI" ); fi
            set -x
            "${CMD[@]}"
            set +x

        volumeMounts:
          - name: data
            mountPath: /data

    - name: upload-s3
      inputs:
        parameters:
          - name: image
          - name: output_zarr
          - name: s3_endpoint
          - name: s3_bucket
          - name: s3_key
          - name: s3_access_key
          - name: s3_secret_key
      container:
        image: "{{inputs.parameters.image}}"
        imagePullPolicy: IfNotPresent
        envFrom:
          - secretRef:
              name: ovh-s3-creds
              optional: true
  # Credentials are preferably provided via the ovh-s3-creds Secret.
  # As a fallback, submit can pass s3_access_key/s3_secret_key params.
        command: [/bin/bash, -lc]
        args:
          - |
            set -euo pipefail

            OUT="{{inputs.parameters.output_zarr}}"
            ENDPOINT="{{inputs.parameters.s3_endpoint}}"
            BUCKET="{{inputs.parameters.s3_bucket}}"
            KEY="{{inputs.parameters.s3_key}}"
            ACCESS_KEY="{{inputs.parameters.s3_access_key}}"
            SECRET_KEY="{{inputs.parameters.s3_secret_key}}"

            if [ -z "$BUCKET" ]; then
              echo "No s3_bucket provided; skipping upload."
              exit 0
            fi

            # Default key from output_zarr if not provided
            if [ -z "$KEY" ]; then
              BASENAME="$(basename "$OUT")"
              KEY="$BASENAME"
            fi

            # Credentials may come from a Kubernetes Secret (envFrom) or from provided parameters
            if [ -n "$ACCESS_KEY" ] && [ -n "$SECRET_KEY" ]; then
              export AWS_ACCESS_KEY_ID="$ACCESS_KEY"
              export AWS_SECRET_ACCESS_KEY="$SECRET_KEY"
            fi

            echo "Uploading (sync) $OUT to s3://$BUCKET/$KEY via $ENDPOINT ..."
            # Ensure KEY has no leading slash
            KEY="${KEY#/}"
            set -x
            aws s3 sync "$OUT/" "s3://$BUCKET/$KEY/" --endpoint-url "$ENDPOINT" --only-show-errors
            set +x
            echo "Upload complete."
        volumeMounts:
          - name: data
            mountPath: /data

    - name: register-stac
      inputs:
        parameters:
          - name: image
          - name: output_zarr
          - name: register_href
          - name: register_url
          - name: register_collection
          - name: register_bearer_token
          - name: s3_endpoint
          - name: s3_bucket
          - name: s3_key
      container:
        image: "{{inputs.parameters.image}}"
        imagePullPolicy: IfNotPresent
        command: [/bin/bash, -lc]
        args:
          - |
            set -euo pipefail

            OUT="{{inputs.parameters.output_zarr}}"
            HREF_OVERRIDE="{{inputs.parameters.register_href}}"
            URL="{{inputs.parameters.register_url}}"
            COLLECTION="{{inputs.parameters.register_collection}}"
            TOKEN="{{inputs.parameters.register_bearer_token}}"
            ENDPOINT="{{inputs.parameters.s3_endpoint}}"
            BUCKET="{{inputs.parameters.s3_bucket}}"
            KEY="{{inputs.parameters.s3_key}}"

            if [ -z "$URL" ]; then
              echo "No register_url provided; skipping STAC registration."
              exit 0
            fi
            if [ -z "$COLLECTION" ]; then
              echo "register_url provided but register_collection is empty; skipping registration."
              exit 0
            fi

            ITEM_ID="$(basename "$OUT" | sed 's/\.[^.]*$//')"
            if [ -n "$HREF_OVERRIDE" ]; then
              HREF_PLACEHOLDER="$HREF_OVERRIDE"
            else
              if [ -n "$BUCKET" ]; then
                if [ -z "$KEY" ]; then
                  KEY="$(basename "$OUT")"
                fi
                # Prefer a stable https URL for clients. OVH public URL layout for S3 is region dependent; allow endpoint-based path format as fallback.
                HREF_PLACEHOLDER="${ENDPOINT%/}/$BUCKET/$KEY"
              else
                HREF_PLACEHOLDER="$OUT"
              fi
            fi
            ITEM_JSON="/data/${ITEM_ID}.item.json"

            echo "=== build minimal STAC Item ==="
            printf '%s\n' \
              '{' \
              '  "type": "Feature",' \
              '  "stac_version": "1.0.0",' \
              "  \"id\": \"${ITEM_ID}\"," \
              "  \"collection\": \"${COLLECTION}\"," \
              '  "geometry": null,' \
              '  "bbox": null,' \
              '  "properties": {},' \
              '  "assets": {' \
              '    "data": {' \
              "      \"href\": \"${HREF_PLACEHOLDER}\"," \
              '      "type": "application/vnd+zarr",' \
              '      "roles": ["data"],' \
              '      "title": "GeoZarr dataset"' \
              '    }' \
              '  },' \
              '  "links": []' \
              '}' \
              > "$ITEM_JSON"

            echo "Wrote $ITEM_JSON"

            echo "=== POST to STAC Transactions (if supported) ==="
            HDRS=("Content-Type: application/json")
            if [ -n "$TOKEN" ]; then
              HDRS+=("Authorization: Bearer $TOKEN")
            fi
            set -x
            curl -fsS -X POST "${URL%/}/collections/${COLLECTION}/items" \
              "${HDRS[@]/#/-H }" \
              --data-binary "@${ITEM_JSON}" \
              -o /data/register-response.json
            set +x
            echo "Registration response saved to /data/register-response.json"
        volumeMounts:
          - name: data
            mountPath: /data

  ttlStrategy:
    secondsAfterCompletion: 300
  podGC:
    strategy: OnPodCompletion
